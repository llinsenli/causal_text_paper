{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review on DoubleLingo: Causal Estimation with Large Language Models\n",
    "### Motivation\n",
    "The motivation of the paper addresses a significant limitation in the application of large language models (LLMs) in natural language processing (NLP). Despite the transformative impact of LLMs on NLP, as highlighted by seminal works such as Vaswani et al. (2017) and Min et al. (2023), these models exhibit inherent weaknesses as estimators of causal parameters. Due to both explicit and implicit regularization processes inherent in their training (referenced in Neyshabur, 2017, and Chernozhukov et al., 2018), LLMs fail to consistently converge to the true causal effects. Specifically, utilizing a popular LLM like BERT (Devlin et al., 2019) to predict propensity scores $P(A | T)$ inherently introduces bias, making these models unsuitable for direct causal inference without adjustments.\n",
    "\n",
    "### Model Setup\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/causal_graph.jpg\" alt=\"Causal Graph\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "1. **Variables**:\n",
    "   - **$A$**: Binary variable indicating whether a patient receives antibiotics $(1)$ or not $(0)$.\n",
    "   - **$Y$**: Binary outcome variable denoting whether the disease progresses $(1)$ or not $(0)$.\n",
    "   - **$T$**: Patient medical records.\n",
    "   - **$C$**: Confounding variables contained in the records.\n",
    "\n",
    "2. **Equations**:\n",
    "   - **Equation $(8)$**: $Y = A\\theta_0 + g_0(T) + U$, where $\\theta_0$ is the ATE, $g_0(T)$ is a function of the confounders, and $U$ is the error term with $E[U | T, A] = 0$.\n",
    "   - **Equation $(9)$**: $A = m_0(T) + V$, where $m_0(T)$ models the probability of treatment given the covariates, and $V$ is the error term with $E[V | T] = 0$.\n",
    "\n",
    "### Double Machine Learning Approach\n",
    "1. **Orthogonalization**:\n",
    "   - Compute the residuals $\\hat{V} = A - \\hat{m}_0(T)$, where $\\hat{m}_0$ is the machine learning estimator of $m_0$. This step removes the influence of $T$ from $A$, focusing directly on the effect of treatment, thereby isolate the effect of $A$ from the confounding influence of $T$.\n",
    "\n",
    "2. **Sample Splitting**:\n",
    "   - The dataset of size $N$ is split into two equal parts: main sample (indices $I$) and auxiliary sample (indices $I_C$). The estimators $\\hat{m}_0$ and $\\hat{g}_0$ are trained on $I_C$ and then used to estimate $\\theta_0$ on $I$.\n",
    "\n",
    "3. **Estimation of $\\theta_0$**:\n",
    "   - **Treatment Residuals $\\hat{V}$**: Derived by predicting $A$ (treatment) from covariates $T$, where $\\hat{V} = A - \\hat{m}_0(T)$. These residuals isolate the effect of treatment from the confounding variables.\n",
    "   - **Outcome Adjusted for Covariates**: Calculated as $\\hat{W} = Y - \\hat{g}_0(T)$, adjusting the outcome $Y$ for influences of $T$, leaving the portion of $Y$ unexplained by $T$.\n",
    "   - By fitting the residuals $\\hat{V}$ against the adjusted outcome $\\hat{W}$ in a linear model:\n",
    "         $$\n",
    "            \\hat{W} = \\theta_0 \\hat{V}\n",
    "         $$\n",
    "   -  The regression provides an estimate for $\\theta_0$, representing the causal effect of treatment independent of $T$ .The estimator $\\hat{\\theta}_0$ is calculated as follows:\n",
    "     $$\n",
    "     \\hat{\\theta}_0 = \\left(\\frac{1}{n} \\sum_{i \\in I} \\hat{V}_{i} A_i \\right)^{-1} \\left(\\frac{1}{n} \\sum_{i \\in I} \\hat{V}_{i} (Y_i - \\hat{g}_0(T_i)) \\right)\n",
    "     $$\n",
    "   - This formula corrects for the effects of $T$ on both $A$ and $Y$, focusing solely on the treatment effect.\n",
    "\n",
    "4. **Error Decomposition**:\n",
    "   - **Term $A$**: Represents the scaled estimation error and converges to a Gaussian distribution, indicating the stability of the estimation process under the model.\n",
    "   - **Term $B$**: Accounts for the regularization bias due to errors in the machine learning estimators and diminishes with appropriate rates of convergence.\n",
    "   - **Term $C$**: Accounts for the model misfit and is controlled through sample-splitting, ensuring it remains small.\n",
    "\n",
    "\n",
    "### Model for Faster Converging\n",
    "A concern arises with the requirement that the two machine learning estimators must converge at $n^{-1/4}$ to obtain a desired $\\sqrt{n}$-consistent estimation of $\\theta_0$. Research on the convergence rate of encoder-based transformer classifiers such as BERT, specifically in the context of semiparametric inference, remains sparse. To address potential convergence issues:\n",
    "\n",
    "- **BERT+Adapter**: Instead of full model fine-tuning, they use parameter-efficient transfer learning through adapters, as proposed by ```Houlsby et al. (2019)```. This adaptation allows for a more focused and potentially faster convergence without comprehensive theoretical bounds currently established.\n",
    "\n",
    "![BERT Adapter Model](images/bert_adapter.jpg)\n",
    "\n",
    "- **Embedding+FFN**:  Rather than fully fine-tuning BERT, they propose fine-tuning a feedforward layer on top of BERT’s pre-trained [CLS] encoding. This specific encoding, originally trained for next-sentence prediction, may not provide semantically rich sentence representations. Instead, they  employ embeddings from pre-trained sentence transformers (Reimers and Gurevych, 2019), which offer more nuanced semantic understanding. \n",
    "   - ```all-mpnet-base-v2```: based on MPNet (Song et al., 2020) and fine-tuned on over 1 billion sentence pairs including paper abstracts\n",
    "   - ```SPECTER```: pre-trained on a dataset of scientific paper titles and abstracts\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "- Main Findings\n",
    "   - Among ```BERT+Adapter```, ```MPNetV2+FFN```, ```SPECTER+FFN```, TF-IDF+FFN and Oracle (C), their three DoubleLingo estimators obtain the lowest AT E relative absolute error 0.103 compared to the Oracle 0.115. (Oracle (Outcome regressions) with full access to the (otherwise unobserved) C variable) \n",
    "   - While theoretically, adjusting for variable $C$ should perfectly account for all confounding between treatment $A$ and outcome $Y$, in practice, $C$ does not capture all the complexities of the relationship. The passage suggests that other variables, particularly $T$, might also influence $Y$, and including $T$ in the model can lead to better estimation accuracy. The model \"DoubleLingo\" that incorporates $T$ outperforms the theoretically ideal \"Oracle\" model, indicating that in real-world applications, it's crucial to consider more than just the theoretically perfect set of confounders.\n",
    "\n",
    "- Convergence Experiment\n",
    "   - They obtained rough estimates that BERT+Adapter, MPNetV2+FFN, SPECTER+FFN, and TF-IDF+FFN converge respectively at $n^{-0.57}$, $n^{-0.64}$, $n^{-0.67}$, and $n^{-0.56}$, all faster than the desired $n^{-0.25}$ rate.\n",
    "   - We can also see as the sample size increase, the estimation accuracy is better. This is align with the consistency. \n",
    "\n",
    "\n",
    "- Note\n",
    "   - According to their Table 2 in Appendix B, the predictive accuracy (on $m_o$ and $g_0$) alone does not directly contribute to a more accurate estimation```(Wood-Doughty et al., 2018)```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>T</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thyroid function and prevalent and incident me...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An open-system quantum simulator with trapped ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On Thermodynamic Interpretation of Copula Entr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X  Y  T  C\n",
       "0  Thyroid function and prevalent and incident me...  0  0  0\n",
       "1  An open-system quantum simulator with trapped ...  0  1  1\n",
       "2  On Thermodynamic Interpretation of Copula Entr...  0  1  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# data = {\n",
    "#     'text': [\n",
    "#         \"Medical record 1\", \"Medical record 2\", \"Medical record 3\", \n",
    "#         \"Medical record 4\", \"Medical record 5\", \"Medical record 6\", \n",
    "#         \"Medical record 7\", \"Medical record 8\", \"Medical record 9\", \"Medical record 10\"\n",
    "#     ],\n",
    "#     'treatment': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
    "#     'outcome': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "# }\n",
    "\n",
    "data = pd.read_csv('data/subpopA_physics_medicine.csv')\n",
    "df = pd.DataFrame(data)\n",
    "df.head(3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE Mean using Double ML: 0.0938\n",
      "Estimated ATE Std Deviation using Double ML: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Example Code for Double Machine Learning###\n",
    "\n",
    "# Convert text to features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['X']).toarray()\n",
    "treatment = df['T'].values\n",
    "outcome = df['Y'].values\n",
    "\n",
    "# Number of iterations\n",
    "n_iterations = 10\n",
    "ate_estimates = []\n",
    "\n",
    "for _ in tqdm(range(n_iterations)):\n",
    "    # Random split into two parts: I and I_C\n",
    "    X_I, X_IC, treatment_I, treatment_IC, outcome_I, outcome_IC = train_test_split(\n",
    "        X, treatment, outcome, test_size=0.5, random_state=np.random.randint(0, 10000))  # Use a varying random state for each iteration\n",
    "\n",
    "    # Train treatment model on I_C\n",
    "    treatment_model = LogisticRegression()\n",
    "    treatment_model.fit(X_IC, treatment_IC)\n",
    "    treatment_preds_I = treatment_model.predict_proba(X_I)[:, 1]\n",
    "\n",
    "    # Train outcome model on I_C\n",
    "    outcome_model = LogisticRegression()\n",
    "    outcome_model.fit(X_IC, outcome_IC)\n",
    "    outcome_preds_I = outcome_model.predict_proba(X_I)[:, 1]\n",
    "\n",
    "    # Calculate residuals on I\n",
    "    treatment_residuals = treatment_I - treatment_preds_I\n",
    "    outcome_residuals = outcome_I - outcome_preds_I\n",
    "\n",
    "    # OLS on residuals to estimate treatment effect\n",
    "    treatment_effect_model = sm.OLS(outcome_residuals, sm.add_constant(treatment_residuals))\n",
    "    results = treatment_effect_model.fit()\n",
    "    ate_estimates.append(results.params[1])\n",
    "\n",
    "# Calculate mean and standard deviation of ATE estimates\n",
    "ate_mean = np.mean(ate_estimates)\n",
    "ate_std = np.std(ate_estimates)\n",
    "\n",
    "print(f\"Estimated ATE Mean using Double ML: {ate_mean:.4f}\")\n",
    "print(f\"Estimated ATE Std Deviation using Double ML: {ate_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
